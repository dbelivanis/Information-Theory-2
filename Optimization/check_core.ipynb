{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.1",
   "language": "julia"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b52351028b39e813c82a08b1f92f8b08c926f93d8b7c782ac06736618b2486c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-30 20:13:21.081249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\nWARNING: using param.param in module aux_functions conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "include(\"./param.jl\");\n",
    "include(\"./aux_functions.jl\");\n",
    "\n",
    "using Main.aux_functions\n",
    "using Main.param\n",
    "using ADCME\n",
    "using SparseArrays\n",
    "\n",
    "using DelimitedFiles\n",
    "using Dates\n",
    "using Plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "maxiter = 10\n",
    "N_steps = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "maximum iteration: 10\tNumber of steps: 3\n",
      "2.830469177224561e-5C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2021-05-30 20:14:56.150970: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "2021-05-30 20:14:56.158589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n",
      "2021-05-30 20:14:56.201469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\n",
      "pciBusID: 0000:01:00.0\n",
      "2021-05-30 20:14:56.201508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\n",
      "2021-05-30 20:14:56.211543: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\n",
      "2021-05-30 20:14:56.219087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\n",
      "2021-05-30 20:14:56.220982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\n",
      "2021-05-30 20:14:56.229402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\n",
      "2021-05-30 20:14:56.233906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\n",
      "2021-05-30 20:14:56.248932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\n",
      "2021-05-30 20:14:56.249060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2021-05-30 20:14:57.428084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-30 20:14:57.428142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2021-05-30 20:14:57.428158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2021-05-30 20:14:57.428659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2919 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "print(\"maximum iteration: \",maxiter,\"\\t\",\"Number of steps: \",N_steps,\"\\n\")\n",
    "param_model_val = param_model(N_steps=N_steps);\n",
    "tf_variables, h_t, q_t_x, q_t_y = Darcy_flow_solver(param_model_val);\n",
    "\n",
    "loss,dw_2, opt_ADAM, opt_LFGS, opt_ADAM_sum, opt_LFGS_sum, diff_eval,p_pre_soft_max, p = Info_upscale(tf_variables,param_model_val,q_t_x, q_t_y,maxiter)\n",
    "\n",
    "sess = Session(); init(sess);\n",
    "\n",
    "\n",
    "global N_k_dis_ = 4\n",
    "global T_exp = -2\n",
    "T_=  10.0 .^ -T_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20:15-2\t100.0\t0.00046288032394218285\t5.515222914819479\t\t4\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "┌ Info: Optimization starts...\n",
      "└ @ ADCME C:\\Users\\dbeli\\.julia\\packages\\ADCME\\cupNK\\src\\optim.jl:332\n",
      "iter 0, current loss=0.0008691973280023414\n",
      "iter 1, current loss=0.0007249220067779027\n",
      "================ STEP 0 ===============\n",
      "iter 2, current loss=0.00028146394462546533\n",
      "================ STEP 1 ===============\n",
      "iter 3, current loss=0.00014079664604997824\n",
      "================ STEP 2 ===============\n",
      "iter 4, current loss=5.907687142474151e-5\n",
      "================ STEP 3 ===============\n",
      "iter 5, current loss=2.5842817503479707e-5\n",
      "================ STEP 4 ===============\n",
      "iter 6, current loss=1.2100122809519152e-5\n",
      "================ STEP 5 ===============\n",
      "iter 7, current loss=7.335415305772748e-6\n",
      "================ STEP 6 ===============\n",
      "iter 8, current loss=5.946362146620959e-6\n",
      "================ STEP 7 ===============\n",
      "iter 9, current loss=5.63392657542866e-6\n",
      "================ STEP 8 ===============\n",
      "iter 10, current loss=5.5777191939213775e-6\n",
      "================ STEP 9 ===============\n",
      "20:15-2\t100.0\t3.790518467727708e-5\t0.21616573029366232\t\t4\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.790518467727708e-5"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\n",
    "save_values(sess,param_model_val,tf_variables,q_t_x, q_t_y,p,\"w\")\n",
    "\n",
    "print_status(sess,loss,diff_eval,T_exp,T_,N_k_dis_,tf_variables)\n",
    "# ScipyOptimizerMinimize(sess, opt_LFGS_sum,feed_dict = Dict(tf_variables.lambda => ones(1)*T_,tf_variables.N_k_dis=>8))\n",
    "BFGS!(sess,dw_2*1e5,options=Dict(\"maxiter\"=> maxiter, \"ftol\"=>1e-12, \"gtol\"=>1e-12))\n",
    "\n",
    "save_values(sess,param_model_val,tf_variables,q_t_x, q_t_y,p,\"a\")\n",
    "print_status(sess,loss,diff_eval,T_exp,T_,N_k_dis_,tf_variables)\n",
    "\n",
    "check_diff = run(sess,diff_eval,feed_dict = Dict(tf_variables.lambda => ones(1)*T_,tf_variables.N_k_dis=>N_k_dis_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20:15-2\t100.0\t3.790518467727708e-5\t0.21616573029366232\t\t4\n",
      "20:15-2\t100.0\t3.790518467727708e-5\t0.21616573029366232\t\t4\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "┌ Info: Optimization starts...\n",
      "└ @ ADCME C:\\Users\\dbeli\\.julia\\packages\\ADCME\\cupNK\\src\\optim.jl:332\n",
      "iter 0, current loss=21616.57302937715\n",
      "iter 1, current loss=11646.570000340533\n",
      "================ STEP 0 ===============\n",
      "iter 2, current loss=8126.912512542144\n",
      "================ STEP 1 ===============\n",
      "iter 3, current loss=7532.866505348795\n",
      "================ STEP 2 ===============\n",
      "iter 4, current loss=7193.44502340591\n",
      "================ STEP 3 ===============\n",
      "iter 5, current loss=6942.497732581325\n",
      "================ STEP 4 ===============\n",
      "iter 6, current loss=6407.44319158724\n",
      "================ STEP 5 ===============\n",
      "iter 7, current loss=11741.286840021225\n",
      "iter 8, current loss=6224.539784312788\n",
      "================ STEP 6 ===============\n",
      "iter 9, current loss=6021.988096621494\n",
      "================ STEP 7 ===============\n",
      "iter 10, current loss=5990.621264699857\n",
      "================ STEP 8 ===============\n",
      "iter 11, current loss=5989.109458061637\n",
      "iter 12, current loss=5978.029626399815\n",
      "================ STEP 9 ===============\n",
      "iter 13, current loss=5970.984622214218\n",
      "================ STEP 10 ===============\n",
      "iter 14, current loss=5966.79808463035\n",
      "================ STEP 11 ===============\n",
      "iter 15, current loss=5964.4157081890735\n",
      "================ STEP 12 ===============\n",
      "iter 16, current loss=5963.410530027255\n",
      "================ STEP 13 ===============\n",
      "iter 17, current loss=5962.297702922046\n",
      "================ STEP 14 ===============\n",
      "iter 18, current loss=5961.32102860429\n",
      "================ STEP 15 ===============\n",
      "iter 19, current loss=5959.9453411676\n",
      "================ STEP 16 ===============\n",
      "iter 20, current loss=5958.038169154911\n",
      "================ STEP 17 ===============\n",
      "iter 21, current loss=5957.841256176534\n",
      "================ STEP 18 ===============\n",
      "iter 22, current loss=5955.339635328412\n",
      "================ STEP 19 ===============\n",
      "iter 23, current loss=5954.764403761351\n",
      "================ STEP 20 ===============\n",
      "iter 24, current loss=5953.992537258868\n",
      "================ STEP 21 ===============\n",
      "iter 25, current loss=5953.748918362118\n",
      "================ STEP 22 ===============\n",
      "iter 26, current loss=5952.854272720373\n",
      "================ STEP 23 ===============\n",
      "iter 27, current loss=5952.534975542351\n",
      "================ STEP 24 ===============\n",
      "iter 28, current loss=5952.258218835271\n",
      "================ STEP 25 ===============\n",
      "iter 29, current loss=5952.104875113502\n",
      "================ STEP 26 ===============\n",
      "iter 30, current loss=5951.998194712555\n",
      "================ STEP 27 ===============\n",
      "iter 31, current loss=5951.9715205228795\n",
      "================ STEP 28 ===============\n",
      "iter 32, current loss=5951.940123317648\n",
      "================ STEP 29 ===============\n",
      "iter 33, current loss=5951.893091917709\n",
      "================ STEP 30 ===============\n",
      "iter 34, current loss=5951.846264226213\n",
      "================ STEP 31 ===============\n",
      "iter 35, current loss=5951.802365807296\n",
      "================ STEP 32 ===============\n",
      "iter 36, current loss=5951.771274476857\n",
      "================ STEP 33 ===============\n",
      "iter 37, current loss=5951.7348350885795\n",
      "================ STEP 34 ===============\n",
      "iter 38, current loss=5951.70667767769\n",
      "================ STEP 35 ===============\n",
      "iter 39, current loss=5951.667892731861\n",
      "================ STEP 36 ===============\n",
      "iter 40, current loss=5951.608234226397\n",
      "================ STEP 37 ===============\n",
      "iter 41, current loss=5951.586345669681\n",
      "================ STEP 38 ===============\n",
      "iter 42, current loss=5951.551393076395\n",
      "================ STEP 39 ===============\n",
      "iter 43, current loss=5951.5340820891715\n",
      "================ STEP 40 ===============\n",
      "iter 44, current loss=5951.517804760156\n",
      "================ STEP 41 ===============\n",
      "iter 45, current loss=5951.50583108323\n",
      "================ STEP 42 ===============\n",
      "iter 46, current loss=5951.493456515267\n",
      "================ STEP 43 ===============\n",
      "iter 47, current loss=5951.485184113501\n",
      "================ STEP 44 ===============\n",
      "iter 48, current loss=5951.470054305316\n",
      "================ STEP 45 ===============\n",
      "iter 49, current loss=5951.458274762494\n",
      "================ STEP 46 ===============\n",
      "iter 50, current loss=5951.4486262350965\n",
      "================ STEP 47 ===============\n",
      "iter 51, current loss=5951.436774591591\n",
      "================ STEP 48 ===============\n",
      "iter 52, current loss=5951.430462944116\n",
      "================ STEP 49 ===============\n",
      "iter 53, current loss=5951.422426504998\n",
      "================ STEP 50 ===============\n",
      "iter 54, current loss=5951.4172538710245\n",
      "================ STEP 51 ===============\n",
      "iter 55, current loss=5951.41599170985\n",
      "================ STEP 52 ===============\n",
      "iter 56, current loss=5951.412446604387\n",
      "================ STEP 53 ===============\n",
      "iter 57, current loss=5951.410167117229\n",
      "================ STEP 54 ===============\n",
      "iter 58, current loss=5951.407508459738\n",
      "================ STEP 55 ===============\n",
      "iter 59, current loss=5951.404488806618\n",
      "================ STEP 56 ===============\n",
      "iter 60, current loss=5951.405659961832\n",
      "iter 61, current loss=5951.402711674699\n",
      "================ STEP 57 ===============\n",
      "iter 62, current loss=5951.399833122408\n",
      "================ STEP 58 ===============\n",
      "iter 63, current loss=5951.398497378443\n",
      "================ STEP 59 ===============\n",
      "iter 64, current loss=5951.396322765145\n",
      "================ STEP 60 ===============\n",
      "iter 65, current loss=5951.394005986302\n",
      "================ STEP 61 ===============\n",
      "iter 66, current loss=5951.391358684323\n",
      "================ STEP 62 ===============\n",
      "iter 67, current loss=5951.389429954355\n",
      "================ STEP 63 ===============\n",
      "iter 68, current loss=5951.388392613923\n",
      "================ STEP 64 ===============\n",
      "iter 69, current loss=5951.387483027754\n",
      "================ STEP 65 ===============\n",
      "iter 70, current loss=5951.386506194529\n",
      "================ STEP 66 ===============\n",
      "iter 71, current loss=5951.385554714667\n",
      "================ STEP 67 ===============\n",
      "iter 72, current loss=5951.384476724881\n",
      "================ STEP 68 ===============\n",
      "iter 73, current loss=5951.384639650444\n",
      "iter 74, current loss=5951.3840094051\n",
      "================ STEP 69 ===============\n",
      "iter 75, current loss=5951.383221403711\n",
      "================ STEP 70 ===============\n",
      "iter 76, current loss=5951.382500081941\n",
      "================ STEP 71 ===============\n",
      "iter 77, current loss=5951.3818709956\n",
      "================ STEP 72 ===============\n",
      "iter 78, current loss=5951.380823272947\n",
      "================ STEP 73 ===============\n",
      "iter 79, current loss=5951.380400937175\n",
      "================ STEP 74 ===============\n",
      "iter 80, current loss=5951.3799832441955\n",
      "================ STEP 75 ===============\n",
      "iter 81, current loss=5951.379757116234\n",
      "================ STEP 76 ===============\n",
      "iter 82, current loss=5951.3793076505945\n",
      "================ STEP 77 ===============\n",
      "iter 83, current loss=5951.3789614052985\n",
      "================ STEP 78 ===============\n",
      "iter 84, current loss=5951.3787950975775\n",
      "================ STEP 79 ===============\n",
      "iter 85, current loss=5951.378535782667\n",
      "================ STEP 80 ===============\n",
      "iter 86, current loss=5951.378373252693\n",
      "================ STEP 81 ===============\n",
      "iter 87, current loss=5951.378278939218\n",
      "================ STEP 82 ===============\n",
      "iter 88, current loss=5951.378230413504\n",
      "================ STEP 83 ===============\n",
      "iter 89, current loss=5951.3781455393355\n",
      "================ STEP 84 ===============\n",
      "iter 90, current loss=5951.377998924158\n",
      "================ STEP 85 ===============\n",
      "iter 91, current loss=5951.377836626002\n",
      "================ STEP 86 ===============\n",
      "iter 92, current loss=5951.37769758602\n",
      "================ STEP 87 ===============\n",
      "iter 93, current loss=5951.377444528526\n",
      "================ STEP 88 ===============\n",
      "iter 94, current loss=5951.377258197437\n",
      "================ STEP 89 ===============\n",
      "iter 95, current loss=5951.37708496186\n",
      "================ STEP 90 ===============\n",
      "iter 96, current loss=5951.376748652992\n",
      "================ STEP 91 ===============\n",
      "iter 97, current loss=5951.376671746267\n",
      "================ STEP 92 ===============\n",
      "iter 98, current loss=5951.376541514222\n",
      "================ STEP 93 ===============\n",
      "iter 99, current loss=5951.37635705337\n",
      "================ STEP 94 ===============\n",
      "iter 100, current loss=5951.376158939662\n",
      "================ STEP 95 ===============\n",
      "iter 101, current loss=5951.375826326057\n",
      "================ STEP 96 ===============\n",
      "iter 102, current loss=5951.377142853649\n",
      "iter 103, current loss=5951.375716666932\n",
      "================ STEP 97 ===============\n",
      "iter 104, current loss=5951.3754535959815\n",
      "================ STEP 98 ===============\n",
      "iter 105, current loss=5951.375315978517\n",
      "================ STEP 99 ===============\n",
      "20:16-2\t100.0\t3.678575221367102e-5\t0.059513753159772136\t\t4\n",
      "function for update K:3.678575221367102e-5\t4\n",
      "20:16-1.9\t79.43282347242814\t0.00010586511537386873\t0.07492388668122996\t\t4\n",
      "20:16-1.9\t79.43282347242814\t0.00010586511537386873\t0.07492388668122996\t\t4\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "┌ Info: Optimization starts...\n",
      "└ @ ADCME C:\\Users\\dbeli\\.julia\\packages\\ADCME\\cupNK\\src\\optim.jl:332\n",
      "iter 0, current loss=7492.388668128712\n",
      "iter 1, current loss=16612.339454610246\n",
      "iter 2, current loss=7492.685339036903\n",
      "iter 3, current loss=7492.349607636134\n",
      "================ STEP 0 ===============\n",
      "iter 4, current loss=7492.344054933826\n",
      "================ STEP 1 ===============\n",
      "iter 5, current loss=7492.340019601652\n",
      "================ STEP 2 ===============\n",
      "iter 6, current loss=7492.3390855124535\n",
      "================ STEP 3 ===============\n",
      "iter 7, current loss=7492.338391118287\n",
      "================ STEP 4 ===============\n",
      "iter 8, current loss=7492.33813232572\n",
      "================ STEP 5 ===============\n",
      "iter 9, current loss=7492.337946599243\n",
      "================ STEP 6 ===============\n",
      "iter 10, current loss=7492.337823672299\n",
      "================ STEP 7 ===============\n",
      "iter 11, current loss=7492.337733205937\n",
      "================ STEP 8 ===============\n",
      "iter 12, current loss=7492.337603819522\n",
      "================ STEP 9 ===============\n",
      "iter 13, current loss=7492.337620477888\n",
      "iter 14, current loss=7492.337534711614\n",
      "================ STEP 10 ===============\n",
      "iter 15, current loss=7492.337453387311\n",
      "================ STEP 11 ===============\n",
      "iter 16, current loss=7492.337382797194\n",
      "================ STEP 12 ===============\n",
      "iter 17, current loss=7492.337307919181\n",
      "================ STEP 13 ===============\n",
      "iter 18, current loss=7492.337271308482\n",
      "================ STEP 14 ===============\n",
      "iter 19, current loss=7492.337180545006\n",
      "================ STEP 15 ===============\n",
      "iter 20, current loss=7492.337141380694\n",
      "================ STEP 16 ===============\n",
      "iter 21, current loss=7492.337089837787\n",
      "================ STEP 17 ===============\n",
      "iter 22, current loss=7492.337061513315\n",
      "================ STEP 18 ===============\n",
      "iter 23, current loss=7492.33698493742\n",
      "================ STEP 19 ===============\n",
      "iter 24, current loss=7492.336924813886\n",
      "================ STEP 20 ===============\n",
      "iter 25, current loss=7492.336885554067\n",
      "================ STEP 21 ===============\n",
      "iter 26, current loss=7492.3368415063\n",
      "================ STEP 22 ===============\n",
      "iter 27, current loss=7492.336793416795\n",
      "================ STEP 23 ===============\n",
      "iter 28, current loss=7492.3367636768335\n",
      "================ STEP 24 ===============\n",
      "iter 29, current loss=7492.336702394003\n",
      "================ STEP 25 ===============\n",
      "iter 30, current loss=7492.336638985074\n",
      "================ STEP 26 ===============\n",
      "iter 31, current loss=7492.336573108662\n",
      "================ STEP 27 ===============\n",
      "iter 32, current loss=7492.336457444024\n",
      "================ STEP 28 ===============\n",
      "iter 33, current loss=7492.336386652425\n",
      "================ STEP 29 ===============\n",
      "iter 34, current loss=7492.3363488147015\n",
      "================ STEP 30 ===============\n",
      "iter 35, current loss=7492.336299910814\n",
      "================ STEP 31 ===============\n",
      "iter 36, current loss=7492.336247308016\n",
      "================ STEP 32 ===============\n",
      "iter 37, current loss=7492.336181113181\n",
      "================ STEP 33 ===============\n",
      "iter 38, current loss=7492.336190402455\n",
      "iter 39, current loss=7492.336151574552\n",
      "================ STEP 34 ===============\n",
      "iter 40, current loss=7492.336109182662\n",
      "================ STEP 35 ===============\n",
      "iter 41, current loss=7492.336079230275\n",
      "================ STEP 36 ===============\n",
      "iter 42, current loss=7492.336060880987\n",
      "================ STEP 37 ===============\n",
      "iter 43, current loss=7492.336041017474\n",
      "================ STEP 38 ===============\n",
      "iter 44, current loss=7492.336035356529\n",
      "================ STEP 39 ===============\n",
      "iter 45, current loss=7492.33602662671\n",
      "================ STEP 40 ===============\n",
      "iter 46, current loss=7492.336018038451\n",
      "================ STEP 41 ===============\n",
      "iter 47, current loss=7492.33600680839\n",
      "================ STEP 42 ===============\n",
      "iter 48, current loss=7492.335993686466\n",
      "================ STEP 43 ===============\n",
      "iter 49, current loss=7492.335965649673\n",
      "================ STEP 44 ===============\n",
      "iter 50, current loss=7492.335949167803\n",
      "================ STEP 45 ===============\n",
      "iter 51, current loss=7492.335925648298\n",
      "================ STEP 46 ===============\n",
      "iter 52, current loss=7492.335891571643\n",
      "================ STEP 47 ===============\n",
      "iter 53, current loss=7492.335882625272\n",
      "================ STEP 48 ===============\n",
      "iter 54, current loss=7492.33587065609\n",
      "================ STEP 49 ===============\n",
      "iter 55, current loss=7492.33585709056\n",
      "================ STEP 50 ===============\n",
      "iter 56, current loss=7492.335850934333\n",
      "================ STEP 51 ===============\n",
      "iter 57, current loss=7492.33583940695\n",
      "================ STEP 52 ===============\n",
      "iter 58, current loss=7492.335831702978\n",
      "================ STEP 53 ===============\n",
      "iter 59, current loss=7492.335825640639\n",
      "================ STEP 54 ===============\n",
      "iter 60, current loss=7492.335820996008\n",
      "================ STEP 55 ===============\n",
      "iter 61, current loss=7492.335818999458\n",
      "================ STEP 56 ===============\n",
      "iter 62, current loss=7492.335815199349\n",
      "================ STEP 57 ===============\n",
      "iter 63, current loss=7492.3358112595415\n",
      "================ STEP 58 ===============\n",
      "iter 64, current loss=7492.335807318242\n",
      "================ STEP 59 ===============\n",
      "iter 65, current loss=7492.335799012576\n",
      "================ STEP 60 ===============\n",
      "iter 66, current loss=7492.335795871211\n",
      "================ STEP 61 ===============\n",
      "iter 67, current loss=7492.335787353482\n",
      "================ STEP 62 ===============\n",
      "iter 68, current loss=7492.335784460801\n",
      "================ STEP 63 ===============\n",
      "iter 69, current loss=7492.335782708484\n",
      "================ STEP 64 ===============\n",
      "iter 70, current loss=7492.335780342746\n",
      "================ STEP 65 ===============\n",
      "iter 71, current loss=7492.335778429715\n",
      "================ STEP 66 ===============\n",
      "iter 72, current loss=7492.335774877142\n",
      "================ STEP 67 ===============\n",
      "iter 73, current loss=7492.335772583587\n",
      "================ STEP 68 ===============\n",
      "iter 74, current loss=7492.335763815756\n",
      "================ STEP 69 ===============\n",
      "iter 75, current loss=7492.335758520771\n",
      "================ STEP 70 ===============\n",
      "iter 76, current loss=7492.335746029524\n",
      "================ STEP 71 ===============\n",
      "iter 77, current loss=7492.335736945795\n",
      "================ STEP 72 ===============\n",
      "iter 78, current loss=7492.33573310922\n",
      "================ STEP 73 ===============\n",
      "iter 79, current loss=7492.335729883246\n",
      "================ STEP 74 ===============\n",
      "iter 80, current loss=7492.335725815476\n",
      "================ STEP 75 ===============\n",
      "iter 81, current loss=7492.335723591488\n",
      "================ STEP 76 ===============\n",
      "iter 82, current loss=7492.335721956581\n",
      "================ STEP 77 ===============\n",
      "iter 83, current loss=7492.335721343931\n",
      "================ STEP 78 ===============\n",
      "iter 84, current loss=7492.335719461141\n",
      "================ STEP 79 ===============\n",
      "iter 85, current loss=7492.335718005139\n",
      "================ STEP 80 ===============\n",
      "iter 86, current loss=7492.335716019928\n",
      "================ STEP 81 ===============\n",
      "iter 87, current loss=7492.335713476286\n",
      "================ STEP 82 ===============\n",
      "iter 88, current loss=7492.3357157611135\n",
      "iter 89, current loss=7492.3357126286555\n",
      "================ STEP 83 ===============\n",
      "iter 90, current loss=7492.335711067883\n",
      "================ STEP 84 ===============\n",
      "iter 91, current loss=7492.335710453524\n",
      "================ STEP 85 ===============\n",
      "iter 92, current loss=7492.335709628484\n",
      "================ STEP 86 ===============\n",
      "iter 93, current loss=7492.33570914899\n",
      "================ STEP 87 ===============\n",
      "iter 94, current loss=7492.335708541429\n",
      "================ STEP 88 ===============\n",
      "iter 95, current loss=7492.335708062232\n",
      "================ STEP 89 ===============\n",
      "iter 96, current loss=7492.335707725558\n",
      "================ STEP 90 ===============\n",
      "iter 97, current loss=7492.335707310528\n",
      "================ STEP 91 ===============\n",
      "iter 98, current loss=7492.335706945531\n",
      "================ STEP 92 ===============\n",
      "iter 99, current loss=7492.335706595846\n",
      "================ STEP 93 ===============\n",
      "iter 100, current loss=7492.335706259423\n",
      "================ STEP 94 ===============\n",
      "iter 101, current loss=7492.335705846057\n",
      "================ STEP 95 ===============\n",
      "iter 102, current loss=7492.335705232666\n",
      "================ STEP 96 ===============\n",
      "iter 103, current loss=7492.335704616497\n",
      "================ STEP 97 ===============\n",
      "iter 104, current loss=7492.335704178916\n",
      "================ STEP 98 ===============\n",
      "iter 105, current loss=7492.335704138114\n",
      "iter 106, current loss=7492.335703782082\n",
      "================ STEP 99 ===============\n",
      "20:16-1.9\t79.43282347242814\t2.344071945523579e-6\t0.07492335703778857\t\t4\n",
      "20:16-1.7999999999999998\t63.0957344480193\t2.344071945523579e-6\t0.09432291811175636\t\t4\n",
      "20:16-1.7999999999999998\t63.0957344480193\t2.344071945523579e-6\t0.09432291811175636\t\t4\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "┌ Info: Optimization starts...\n",
      "└ @ ADCME C:\\Users\\dbeli\\.julia\\packages\\ADCME\\cupNK\\src\\optim.jl:332\n",
      "iter 0, current loss=9432.291811179693\n",
      "iter 1, current loss=27219.727292124142\n",
      "iter 2, current loss=9432.292318941389\n",
      "iter 3, current loss=9432.29181087239\n",
      "================ STEP 0 ===============\n",
      "iter 4, current loss=9432.291810805058\n",
      "================ STEP 1 ===============\n",
      "iter 5, current loss=9432.29181049724\n",
      "================ STEP 2 ===============\n",
      "iter 6, current loss=9432.291810269404\n",
      "================ STEP 3 ===============\n",
      "iter 7, current loss=9432.291810157823\n",
      "================ STEP 4 ===============\n",
      "iter 8, current loss=9432.29181007637\n",
      "================ STEP 5 ===============\n",
      "iter 9, current loss=9432.291809861052\n",
      "================ STEP 6 ===============\n",
      "iter 10, current loss=9432.291809670718\n",
      "================ STEP 7 ===============\n",
      "iter 11, current loss=9432.291809557255\n",
      "================ STEP 8 ===============\n",
      "iter 12, current loss=9432.291809384928\n",
      "================ STEP 9 ===============\n",
      "iter 13, current loss=9432.291809277625\n",
      "================ STEP 10 ===============\n",
      "iter 14, current loss=9432.29180915273\n",
      "================ STEP 11 ===============\n",
      "iter 15, current loss=9432.291808931168\n",
      "================ STEP 12 ===============\n",
      "iter 16, current loss=9432.291808673777\n",
      "================ STEP 13 ===============\n",
      "iter 17, current loss=9432.291808457301\n",
      "================ STEP 14 ===============\n",
      "iter 18, current loss=9432.291808131626\n",
      "================ STEP 15 ===============\n",
      "iter 19, current loss=9432.291807947515\n",
      "================ STEP 16 ===============\n",
      "iter 20, current loss=9432.29180760936\n",
      "================ STEP 17 ===============\n",
      "iter 21, current loss=9432.291807298263\n",
      "================ STEP 18 ===============\n",
      "iter 22, current loss=9432.291807184061\n",
      "================ STEP 19 ===============\n",
      "iter 23, current loss=9432.291806935948\n",
      "================ STEP 20 ===============\n",
      "iter 24, current loss=9432.291807094984\n",
      "iter 25, current loss=9432.291806794723\n",
      "================ STEP 21 ===============\n",
      "iter 26, current loss=9432.291806502119\n",
      "================ STEP 22 ===============\n",
      "iter 27, current loss=9432.291806279238\n",
      "================ STEP 23 ===============\n",
      "iter 28, current loss=9432.291805893672\n",
      "================ STEP 24 ===============\n",
      "iter 29, current loss=9432.291806174308\n",
      "iter 30, current loss=9432.291805773617\n",
      "================ STEP 25 ===============\n",
      "iter 31, current loss=9432.291805606152\n",
      "================ STEP 26 ===============\n",
      "iter 32, current loss=9432.291805444836\n",
      "================ STEP 27 ===============\n",
      "iter 33, current loss=9432.29180535322\n",
      "================ STEP 28 ===============\n",
      "iter 34, current loss=9432.291805252971\n",
      "================ STEP 29 ===============\n",
      "iter 35, current loss=9432.291805144827\n",
      "================ STEP 30 ===============\n",
      "iter 36, current loss=9432.291805084978\n",
      "================ STEP 31 ===============\n",
      "iter 37, current loss=9432.291804955295\n",
      "================ STEP 32 ===============\n",
      "iter 38, current loss=9432.291805414918\n",
      "iter 39, current loss=9432.291804939827\n",
      "================ STEP 33 ===============\n",
      "iter 40, current loss=9432.291804911567\n",
      "================ STEP 34 ===============\n",
      "iter 41, current loss=9432.29180489685\n",
      "================ STEP 35 ===============\n",
      "iter 42, current loss=9432.291804883005\n",
      "================ STEP 36 ===============\n",
      "iter 43, current loss=9432.291804861172\n",
      "================ STEP 37 ===============\n",
      "iter 44, current loss=9432.291804819879\n",
      "================ STEP 38 ===============\n",
      "iter 45, current loss=9432.291804768332\n",
      "================ STEP 39 ===============\n",
      "iter 46, current loss=9432.291804708402\n",
      "================ STEP 40 ===============\n",
      "iter 47, current loss=9432.291804623745\n",
      "================ STEP 41 ===============\n",
      "iter 48, current loss=9432.291804540946\n",
      "================ STEP 42 ===============\n",
      "iter 49, current loss=9432.291804493023\n",
      "================ STEP 43 ===============\n",
      "iter 50, current loss=9432.291804459222\n",
      "================ STEP 44 ===============\n",
      "iter 51, current loss=9432.291804437746\n",
      "================ STEP 45 ===============\n",
      "iter 52, current loss=9432.291804393917\n",
      "================ STEP 46 ===============\n",
      "iter 53, current loss=9432.291804379154\n",
      "================ STEP 47 ===============\n",
      "iter 54, current loss=9432.291804335588\n",
      "================ STEP 48 ===============\n",
      "iter 55, current loss=9432.291804304054\n",
      "================ STEP 49 ===============\n",
      "iter 56, current loss=9432.291804269318\n",
      "================ STEP 50 ===============\n",
      "iter 57, current loss=9432.291804237273\n",
      "================ STEP 51 ===============\n",
      "iter 58, current loss=9432.29180421902\n",
      "================ STEP 52 ===============\n",
      "iter 59, current loss=9432.291804187977\n",
      "================ STEP 53 ===============\n",
      "iter 60, current loss=9432.29180413782\n",
      "================ STEP 54 ===============\n",
      "iter 61, current loss=9432.291804119888\n",
      "================ STEP 55 ===============\n",
      "iter 62, current loss=9432.2918040458\n",
      "================ STEP 56 ===============\n",
      "iter 63, current loss=9432.2918045062\n",
      "iter 64, current loss=9432.291804054674\n",
      "iter 65, current loss=9432.291804065237\n",
      "iter 66, current loss=9432.29180405327\n",
      "iter 67, current loss=9432.291804062224\n",
      "iter 68, current loss=9432.291804047\n",
      "iter 69, current loss=9432.2918040458\n",
      "iter 70, current loss=9432.291804054303\n",
      "iter 71, current loss=9432.2918040458\n",
      "iter 72, current loss=9432.291804041253\n",
      "iter 73, current loss=9432.291804048873\n",
      "iter 74, current loss=9432.291804041253\n",
      "iter 75, current loss=9432.291804058288\n",
      "iter 76, current loss=9432.291804041253\n",
      "iter 77, current loss=9433.043837243911\n",
      "iter 78, current loss=9432.291804047545\n",
      "iter 79, current loss=9432.29180404414\n",
      "================ STEP 57 ===============\n",
      "20:17-1.7999999999999998\t63.0957344480193\t3.5774198807752806e-7\t0.09432291804042617\t\t4\n",
      "20:17-1.6999999999999997\t50.118723362727195\t3.5774198807752806e-7\t0.11874551843566969\t\t4\n",
      "20:17-1.6999999999999997\t50.118723362727195\t3.5774198807752806e-7\t0.11874551843566969\t\t4\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "┌ Info: Optimization starts...\n",
      "└ @ ADCME C:\\Users\\dbeli\\.julia\\packages\\ADCME\\cupNK\\src\\optim.jl:332\n",
      "iter 0, current loss=11874.551843568886\n",
      "iter 1, current loss=45856.90198319523\n",
      "iter 2, current loss=11874.55184358092\n",
      "iter 3, current loss=11874.551843576384\n",
      "iter 4, current loss=11874.55184356691\n",
      "iter 5, current loss=11874.551843585521\n",
      "iter 6, current loss=11874.55184356691\n",
      "================ STEP 0 ===============\n",
      "20:17-1.6999999999999997\t50.118723362727195\t3.577435570353617e-7\t0.11874551843567234\t\t4\n",
      "20:17-1.5999999999999996\t39.81071705534969\t3.577435570353617e-7\t0.1494917506953386\t\t4\n",
      "20:17-1.5999999999999996\t39.81071705534969\t3.577435570353617e-7\t0.1494917506953386\t\t4\n",
      "C:\\Users\\dbeli\\.julia\\adcme\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "┌ Info: Optimization starts...\n",
      "└ @ ADCME C:\\Users\\dbeli\\.julia\\packages\\ADCME\\cupNK\\src\\optim.jl:332\n",
      "iter 0, current loss=14949.175069533447\n",
      "iter 1, current loss=53456.71775867983\n",
      "iter 2, current loss=14949.175069524546\n",
      "iter 3, current loss=14949.175069541598\n",
      "iter 4, current loss=14949.175069524546\n",
      "================ STEP 0 ===============\n",
      "20:17-1.5999999999999996\t39.81071705534969\t4.3451405623495486e-7\t0.1494917506954067\t\t4\n",
      "20:17-1.4999999999999996\t31.62277660168376\t4.3451405623495486e-7\t0.18819896380404275\t\t4\n",
      "20:17-1.4999999999999996\t31.62277660168376\t4.3451405623495486e-7\t0.18819896380404275\t\t4"
     ]
    }
   ],
   "source": [
    "T_exp_final =4\n",
    "\n",
    "while T_exp <= T_exp_final\n",
    "\n",
    "    global T_=  10.0 ^ (-T_exp) \n",
    "\n",
    "    print_status(sess,loss,diff_eval,T_exp,T_,N_k_dis_,tf_variables)\n",
    "    check_diff_ = run(sess,diff_eval,feed_dict = Dict(tf_variables.lambda => ones(1)*T_,tf_variables.N_k_dis=>N_k_dis_))  \n",
    "    print_status(sess,loss,diff_eval,T_exp,T_,N_k_dis_,tf_variables)\n",
    "    # ScipyOptimizerMinimize(sess, opt_LFGS,feed_dict = Dict(tf_variables.lambda => ones(1)*T_,tf_variables.N_k_dis=>N_k_dis_))\n",
    "    BFGS!(sess,loss*1e5,options=Dict(\"maxiter\"=> 100, \"ftol\"=>1e-12, \"gtol\"=>1e-12),feed_dict = Dict(tf_variables.lambda => ones(1)*T_,tf_variables.N_k_dis=>N_k_dis_))\n",
    "\n",
    "    print_status(sess,loss,diff_eval,T_exp,T_,N_k_dis_,tf_variables)\n",
    "    if round(T_exp,digits=2)%1 == 0\n",
    "\n",
    "        save_values(sess,param_model_val,tf_variables,q_t_x, q_t_y,p)\n",
    "        check_diff_ = run(sess,diff_eval,feed_dict = Dict(tf_variables.lambda => ones(1)*T_,tf_variables.N_k_dis=>N_k_dis_))  \n",
    "        global N_k_dis_ = update_K_p(sess,param_model_val,tf_variables,check_diff_,N_k_dis_,p_pre_soft_max)\n",
    "\n",
    "    end\n",
    "\n",
    "    global T_exp += 0.1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}